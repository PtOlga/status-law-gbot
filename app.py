import gradio as gr
import os
from huggingface_hub import InferenceClient
from config.constants import DEFAULT_SYSTEM_MESSAGE
from config.settings import DEFAULT_MODEL, HF_TOKEN
from src.knowledge_base.vector_store import create_vector_store, load_vector_store

# –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å —Ç–æ–∫–µ–Ω–æ–º
client = InferenceClient(
    DEFAULT_MODEL,
    token=HF_TOKEN
)

# –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
context_store = {}

def get_context(message, conversation_id):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    vector_store = load_vector_store()
    if vector_store is None:
        return "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ–∑–¥–∞–π—Ç–µ –µ—ë —Å–Ω–∞—á–∞–ª–∞."
    
    try:
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_docs = vector_store.similarity_search(message, k=3)
        context_text = "\n\n".join([f"–ò–∑ {doc.metadata.get('source', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}: {doc.page_content}" for doc in context_docs])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —ç—Ç–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        context_store[conversation_id] = context_text
        
        return context_text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {str(e)}")
        return ""

def respond(
    message,
    history,
    conversation_id,
    system_message,
    max_tokens,
    temperature,
    top_p,
):
    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä, —Å–æ–∑–¥–∞–µ–º ID
    if not conversation_id:
        import uuid
        conversation_id = str(uuid.uuid4())
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    context = get_context(message, conversation_id)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    full_system_message = system_message
    if context:
        full_system_message += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞:\n{context}"
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è LLM
    messages = [{"role": "system", "content": full_system_message}]
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è API
    for user_msg, bot_msg in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": bot_msg})
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    messages.append({"role": "user", "content": message})
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ API –∏ —Å—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç
    response = ""
    for message in client.chat_completion(
        messages,
        max_tokens=max_tokens,
        stream=True,
        temperature=temperature,
        top_p=top_p,
    ):
        token = message.choices[0].delta.content
        if token:
            response += token
            yield response, conversation_id

def build_kb():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    try:
        success, message = create_vector_store()
        return message
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {str(e)}"

# –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
with gr.Blocks() as demo:
    gr.Markdown("# ü§ñ Status Law Assistant")
    
    conversation_id = gr.State(None)
    
    with gr.Row():
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(label="–ß–∞—Ç")
            
            with gr.Row():
                msg = gr.Textbox(
                    label="–í–∞—à –≤–æ–ø—Ä–æ—Å",
                    placeholder="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å...",
                    scale=4
                )
                submit_btn = gr.Button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å", variant="primary")
        
        with gr.Column(scale=1):
            gr.Markdown("### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π")
            build_kb_btn = gr.Button("–°–æ–∑–¥–∞—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π", variant="primary")
            kb_status = gr.Textbox(label="–°—Ç–∞—Ç—É—Å –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π", interactive=False)
            
            gr.Markdown("### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞—Ç–∞")
            system_message = gr.Textbox(
                label="–°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ", 
                value=DEFAULT_SYSTEM_MESSAGE,
                lines=5
            )
            max_tokens = gr.Slider(
                minimum=1, 
                maximum=2048, 
                value=512, 
                step=1, 
                label="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤"
            )
            temperature = gr.Slider(
                minimum=0.1, 
                maximum=2.0, 
                value=0.7, 
                step=0.1, 
                label="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞"
            )
            top_p = gr.Slider(
                minimum=0.1, 
                maximum=1.0, 
                value=0.95, 
                step=0.05, 
                label="Top-p (nucleus sampling)"
            )
            
            clear_btn = gr.Button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞")
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
    msg.submit(
        respond, 
        [msg, chatbot, conversation_id, system_message, max_tokens, temperature, top_p], 
        [chatbot, conversation_id]
    )
    submit_btn.click(
        respond, 
        [msg, chatbot, conversation_id, system_message, max_tokens, temperature, top_p], 
        [chatbot, conversation_id]
    )
    build_kb_btn.click(build_kb, None, kb_status)
    clear_btn.click(lambda: ([], None), None, [chatbot, conversation_id])

# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    if not os.path.exists(os.path.join("data", "vector_store", "index.faiss")):
        print("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞–π—Ç–µ –µ—ë —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.")
    
    demo.launch()
